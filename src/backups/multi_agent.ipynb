{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ [\"OPENAL_API_KEY\"] =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "\n",
    "# 1. Trait Persona 생성 Agent\n",
    "def create_trait_personas(essay, prompt, max_score, min_score, traits=None):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    auto_trait_prompt = ChatPromptTemplate.from_messages([\n",
    "        HumanMessage(content=f\"\"\"\n",
    "### Task\n",
    "Given the following essay and prompt, suggest all possible evaluation traits that would be appropriate for assessing this essay.\n",
    "\n",
    "Then, **group** the suggested traits such that:\n",
    "- Traits that are closely related and should be evaluated together are grouped under a single agent.\n",
    "- Traits that are independent are assigned to separate agents.\n",
    "\n",
    "**Reason for Grouping:**\n",
    "Grouping related traits together ensures consistency, avoids redundant evaluations, and maintains contextual coherence, leading to more efficient and accurate assessments.\n",
    "\n",
    "### For Each Trait\n",
    "For each suggested trait, describe the detailed evaluation criteria that the evaluator must consider. The evaluator should look for specific features or aspects of the writing based on the provided rubric.\n",
    "\n",
    "You will also be provided with the score range (e.g., 1-6). When scoring, evaluators must match the writing characteristics against the provided rubric levels.\n",
    "\n",
    "### Output Format (in JSON)\n",
    "{{\n",
    "  \"reasoning\": \"Provide a reasoning why these traits were suggested, considering the essay's purpose, genre, characteristics, etc.\",\n",
    "  \"traits\": [\"Trait1\", \"Trait2\", \"Trait3\", ...],\n",
    "  \"trait_guidelines\": {{\n",
    "    \"Trait1\": [\"Evaluation point 1\", \"Evaluation point 2\", ...],\n",
    "    \"Trait2\": [\"Evaluation point 1\", ...],\n",
    "    ...\n",
    "  }},\n",
    "  \"grouped_traits\": {{\n",
    "    \"GroupName1\": [\"Trait1\", \"Trait2\", ...],\n",
    "    \"GroupName2\": [\"Trait3\"],\n",
    "    ...\n",
    "  }}\n",
    "}}\n",
    "\n",
    "### Essay Prompt\n",
    "{prompt}\n",
    "\n",
    "### Essay\n",
    "{essay}\n",
    "\"\"\")\n",
    "    ])\n",
    "    response = llm.invoke(auto_trait_prompt.format_messages())\n",
    "    traits_json = json.loads(response.content)\n",
    "    traits = traits_json[\"traits\"]\n",
    "    grouped_traits = traits_json[\"grouped_traits\"]\n",
    "\n",
    "\n",
    "    # Trait별 페르소나 생성\n",
    "    trait_personas = {}\n",
    "    for group_name, trait_list in grouped_traits.items():\n",
    "        persona_description = f\"You are an expert evaluator for the trait group: {group_name}. Your job is to evaluate essays based on the following traits: {', '.join(trait_list)}.\"\n",
    "        trait_personas[group_name] = persona_description\n",
    "\n",
    "    return trait_personas\n",
    "\n",
    "# 2. Trait별 채점 Agent\n",
    "class TraitScoringAgent:\n",
    "    def __init__(self, persona_description):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "        self.persona_description = persona_description\n",
    "\n",
    "    def score(self, essay, prompt):\n",
    "        scoring_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.persona_description),\n",
    "            HumanMessage(content=f\"Essay Prompt: {prompt}\\nEssay: {essay}\\nPlease evaluate the essay according to your assigned traits.\")\n",
    "        ])\n",
    "        response = self.llm.invoke(scoring_prompt.format_messages())\n",
    "        return response.content\n",
    "\n",
    "# 3. Holistic Scorer Agent\n",
    "class HolisticScorer:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    def aggregate_scores(self, trait_scores):\n",
    "        holistic_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"You are a holistic essay scorer.\"),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Based on the following trait group scores:\n",
    "            {trait_scores}\n",
    "            Provide a holistic overall score (0-100) and a short reasoning.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        response = self.llm.invoke(holistic_prompt.format_messages())\n",
    "        return response.content\n",
    "\n",
    "# 전체 프로세스\n",
    "class EssayEvaluationSystem:\n",
    "    def __init__(self, traits=None):\n",
    "        self.traits = traits\n",
    "\n",
    "    def evaluate(self, essay, prompt):\n",
    "        # Step 1: Create trait personas\n",
    "        trait_personas = create_trait_personas(essay, prompt, self.traits)\n",
    "\n",
    "        # Step 2: Each group agent scores\n",
    "        trait_scores = {}\n",
    "        for group_name, persona in trait_personas.items():\n",
    "            agent = TraitScoringAgent(persona)\n",
    "            trait_scores[group_name] = agent.score(essay, prompt)\n",
    "\n",
    "        # Step 3: Holistic scoring\n",
    "        holistic_agent = HolisticScorer()\n",
    "        holistic_result = holistic_agent.aggregate_scores(trait_scores)\n",
    "\n",
    "        return trait_scores, holistic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import JsonOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Configuration\n",
    "class EssayEvaluationConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_agents: int = 4,\n",
    "        max_score: int = 10,\n",
    "        min_score: int = 1,\n",
    "        max_holistic_score: int = 100,\n",
    "        min_holistic_score: int = 0,\n",
    "        feedback: bool = True,\n",
    "        model_name: str = \"gpt-4o\"\n",
    "    ):\n",
    "        self.n_agents = n_agents\n",
    "        self.max_score = max_score\n",
    "        self.min_score = min_score\n",
    "        self.max_holistic_score = max_holistic_score\n",
    "        self.min_holistic_score = min_holistic_score\n",
    "        self.feedback = feedback\n",
    "        self.model_name = model_name\n",
    "\n",
    "\n",
    "class PersonaAgent:\n",
    "    \"\"\"Agent that generates a specialized evaluator persona based on essay and prompt\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EssayEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(model=config.model_name, temperature=0.7)\n",
    "        \n",
    "    def generate_personas(self, essay: str, essay_prompt: str) -> List[Dict[str, str]]:\n",
    "        persona_template = \"\"\"\n",
    "        You are an expert at creating specialized personas for evaluating essays.\n",
    "        \n",
    "        Given the following essay and its prompt, create {n_agents} distinct evaluator personas.\n",
    "        \n",
    "        These personas MUST include experts focused on:\n",
    "        1. Essay structure and grammar/mechanics\n",
    "        2. Content and subject matter\n",
    "        3. Alignment with the original prompt requirements\n",
    "        4. The purpose/style of the essay (e.g., persuasive, narrative, etc.)\n",
    "        \n",
    "        Essay Prompt:\n",
    "        {essay_prompt}\n",
    "        \n",
    "        Essay:\n",
    "        {essay}\n",
    "        \n",
    "        For each persona, provide:\n",
    "        1. A name\n",
    "        2. Professional background\n",
    "        3. Specific area of expertise\n",
    "        4. Evaluation focus\n",
    "        \n",
    "        Format your response as a JSON array of persona objects with keys: \"name\", \"background\", \"expertise\", \"focus\"\n",
    "        \"\"\"\n",
    "        \n",
    "        persona_prompt = ChatPromptTemplate.from_template(persona_template)\n",
    "        persona_chain = (\n",
    "            {\"essay_prompt\": RunnablePassthrough(), \"essay\": RunnablePassthrough(), \"n_agents\": lambda _: self.config.n_agents}\n",
    "            | persona_prompt\n",
    "            | self.llm\n",
    "            | JsonOutputParser()\n",
    "        )\n",
    "        \n",
    "        return persona_chain.invoke({\"essay_prompt\": essay_prompt, \"essay\": essay})\n",
    "\n",
    "\n",
    "class RubricAgent:\n",
    "    \"\"\"Agent that generates evaluation rubrics based on personas\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EssayEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(model=config.model_name, temperature=0.5)\n",
    "        \n",
    "    def generate_rubrics(self, personas: List[Dict[str, str]], essay: str, essay_prompt: str) -> List[Dict[str, Any]]:\n",
    "        rubrics = []\n",
    "        \n",
    "        rubric_template = \"\"\"\n",
    "        You are {name}, {background} with expertise in {expertise}.\n",
    "        \n",
    "        Your task is to create a detailed evaluation rubric for assessing an essay. The rubric should focus on your specific area of expertise: {focus}.\n",
    "        \n",
    "        Essay Prompt:\n",
    "        {essay_prompt}\n",
    "        \n",
    "        Essay to Evaluate:\n",
    "        {essay}\n",
    "        \n",
    "        Create a rubric with 3-5 specific traits that evaluate aspects of the essay within your area of expertise.\n",
    "        \n",
    "        For each trait:\n",
    "        - Provide a clear name\n",
    "        - Give a detailed description of what this trait measures\n",
    "        - Include specific criteria for different score levels within the range of {min_score} (lowest) to {max_score} (highest)\n",
    "        \n",
    "        Format your response as a JSON object with the following structure:\n",
    "        {{\n",
    "            \"persona\": {{\n",
    "                \"name\": \"Your persona name\",\n",
    "                \"focus\": \"Your area of focus\"\n",
    "            }},\n",
    "            \"traits\": [\n",
    "                {{\n",
    "                    \"name\": \"Name of trait\",\n",
    "                    \"description\": \"Description of trait\",\n",
    "                    \"criteria\": [\n",
    "                        {{\n",
    "                            \"score\": score_value,\n",
    "                            \"description\": \"What this score means\"\n",
    "                        }},\n",
    "                        ...more score criteria...\n",
    "                    ]\n",
    "                }},\n",
    "                ...more traits...\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        for persona in personas:\n",
    "            rubric_prompt = ChatPromptTemplate.from_template(rubric_template)\n",
    "            rubric_chain = (\n",
    "                {\n",
    "                    \"name\": lambda _: persona[\"name\"],\n",
    "                    \"background\": lambda _: persona[\"background\"],\n",
    "                    \"expertise\": lambda _: persona[\"expertise\"],\n",
    "                    \"focus\": lambda _: persona[\"focus\"],\n",
    "                    \"essay_prompt\": RunnablePassthrough(),\n",
    "                    \"essay\": RunnablePassthrough(),\n",
    "                    \"min_score\": lambda _: self.config.min_score,\n",
    "                    \"max_score\": lambda _: self.config.max_score\n",
    "                }\n",
    "                | rubric_prompt\n",
    "                | self.llm\n",
    "                | JsonOutputParser()\n",
    "            )\n",
    "            \n",
    "            rubric = rubric_chain.invoke({\"essay_prompt\": essay_prompt, \"essay\": essay})\n",
    "            rubrics.append(rubric)\n",
    "            \n",
    "        return rubrics\n",
    "\n",
    "\n",
    "class ScoringAgent:\n",
    "    \"\"\"Agent that scores essays based on persona-specific rubrics\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EssayEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(model=config.model_name, temperature=0.2)\n",
    "        \n",
    "    def generate_scores(self, rubrics: List[Dict[str, Any]], essay: str, essay_prompt: str) -> List[Dict[str, Any]]:\n",
    "        all_scores = []\n",
    "        \n",
    "        scoring_template = \"\"\"\n",
    "        You are {persona_name}, focusing on evaluating essays from the perspective of {persona_focus}.\n",
    "        \n",
    "        Your task is to evaluate the following essay according to your specialized rubric.\n",
    "        \n",
    "        Essay Prompt:\n",
    "        {essay_prompt}\n",
    "        \n",
    "        Essay to Evaluate:\n",
    "        {essay}\n",
    "        \n",
    "        Your evaluation rubric has the following traits:\n",
    "        {traits_json}\n",
    "        \n",
    "        For each trait in your rubric:\n",
    "        1. First, provide detailed reasoning for your evaluation\n",
    "        2. Then, assign a score within the range {min_score} to {max_score}\n",
    "        \n",
    "        Format your response as a JSON object with the following structure:\n",
    "        {{\n",
    "            \"persona\": {{\n",
    "                \"name\": \"Your persona name\",\n",
    "                \"focus\": \"Your area of focus\"\n",
    "            }},\n",
    "            \"trait_scores\": [\n",
    "                {{\n",
    "                    \"trait_name\": \"Name of trait\",\n",
    "                    \"rationale\": \"Detailed explanation of your reasoning\",\n",
    "                    \"score\": assigned_score\n",
    "                }},\n",
    "                ...more trait scores...\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        for rubric in rubrics:\n",
    "            persona_name = rubric[\"persona\"][\"name\"]\n",
    "            persona_focus = rubric[\"persona\"][\"focus\"]\n",
    "            traits_json = json.dumps(rubric[\"traits\"])\n",
    "            \n",
    "            scoring_prompt = ChatPromptTemplate.from_template(scoring_template)\n",
    "            scoring_chain = (\n",
    "                {\n",
    "                    \"persona_name\": lambda _: persona_name,\n",
    "                    \"persona_focus\": lambda _: persona_focus,\n",
    "                    \"essay_prompt\": RunnablePassthrough(),\n",
    "                    \"essay\": RunnablePassthrough(),\n",
    "                    \"traits_json\": lambda _: traits_json,\n",
    "                    \"min_score\": lambda _: self.config.min_score,\n",
    "                    \"max_score\": lambda _: self.config.max_score\n",
    "                }\n",
    "                | scoring_prompt\n",
    "                | self.llm\n",
    "                | JsonOutputParser()\n",
    "            )\n",
    "            \n",
    "            scores = scoring_chain.invoke({\"essay_prompt\": essay_prompt, \"essay\": essay})\n",
    "            all_scores.append(scores)\n",
    "            \n",
    "        return all_scores\n",
    "\n",
    "\n",
    "class MetaScoreAgent:\n",
    "    \"\"\"Agent that aggregates scores from multiple evaluators into a final assessment\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EssayEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(model=config.model_name, temperature=0.3)\n",
    "        \n",
    "    def generate_meta_score(self, all_scores: List[Dict[str, Any]], essay: str, essay_prompt: str) -> Dict[str, Any]:\n",
    "        meta_template = \"\"\"\n",
    "        You are a Meta Evaluator responsible for synthesizing multiple expert evaluations into a coherent final assessment.\n",
    "        \n",
    "        Your task is to review evaluations from {n_agents} expert personas and produce a comprehensive final score and assessment.\n",
    "        \n",
    "        Essay Prompt:\n",
    "        {essay_prompt}\n",
    "        \n",
    "        Essay:\n",
    "        {essay}\n",
    "        \n",
    "        Expert Evaluations:\n",
    "        {evaluations_json}\n",
    "        \n",
    "        Please analyze these evaluations to:\n",
    "        1. Identify unique evaluation traits across all personas\n",
    "        2. Determine appropriate weight for each trait based on its importance\n",
    "        3. Calculate a final holistic score within the range of {min_holistic_score} to {max_holistic_score}\n",
    "        4. {feedback_instruction}\n",
    "        \n",
    "        Format your response as a JSON object with the following structure:\n",
    "        {{\n",
    "            \"trait_summary\": [\n",
    "                {{\n",
    "                    \"trait\": \"Name of trait\",\n",
    "                    \"focus\": \"Related focus area\",\n",
    "                    \"score\": normalized_score,\n",
    "                    \"weight\": assigned_weight\n",
    "                }},\n",
    "                ...more traits...\n",
    "            ],\n",
    "            \"holistic_score\": final_score,\n",
    "            \"feedback\": \"Comprehensive feedback\" // Only if feedback is required\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        feedback_instruction = \"Provide comprehensive feedback with strengths and areas for improvement\" if self.config.feedback else \"No feedback needed\"\n",
    "        \n",
    "        meta_prompt = ChatPromptTemplate.from_template(meta_template)\n",
    "        meta_chain = (\n",
    "            {\n",
    "                \"n_agents\": lambda _: self.config.n_agents,\n",
    "                \"essay_prompt\": RunnablePassthrough(),\n",
    "                \"essay\": RunnablePassthrough(),\n",
    "                \"evaluations_json\": lambda _: json.dumps(all_scores),\n",
    "                \"min_holistic_score\": lambda _: self.config.min_holistic_score,\n",
    "                \"max_holistic_score\": lambda _: self.config.max_holistic_score,\n",
    "                \"feedback_instruction\": lambda _: feedback_instruction\n",
    "            }\n",
    "            | meta_prompt\n",
    "            | self.llm\n",
    "            | JsonOutputParser()\n",
    "        )\n",
    "        \n",
    "        return meta_chain.invoke({\"essay_prompt\": essay_prompt, \"essay\": essay})\n",
    "\n",
    "\n",
    "class EssayEvaluationSystem:\n",
    "    \"\"\"Main system that orchestrates the multi-agent evaluation process\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None, config: Optional[EssayEvaluationConfig] = None):\n",
    "        if api_key:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "            \n",
    "        self.config = config or EssayEvaluationConfig()\n",
    "        self.persona_agent = PersonaAgent(self.config)\n",
    "        self.rubric_agent = RubricAgent(self.config)\n",
    "        self.scoring_agent = ScoringAgent(self.config)\n",
    "        self.meta_agent = MetaScoreAgent(self.config)\n",
    "        \n",
    "    def evaluate_essay(self, essay: str, essay_prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Full evaluation pipeline that returns structured assessment of an essay\"\"\"\n",
    "        \n",
    "        # Step 1: Generate specialized personas\n",
    "        personas = self.persona_agent.generate_personas(essay, essay_prompt)\n",
    "        \n",
    "        # Step 2: Generate rubrics for each persona\n",
    "        rubrics = self.rubric_agent.generate_rubrics(personas, essay, essay_prompt)\n",
    "        \n",
    "        # Step 3: Score essay using each persona's rubric\n",
    "        all_scores = self.scoring_agent.generate_scores(rubrics, essay, essay_prompt)\n",
    "        \n",
    "        # Step 4: Generate meta-assessment with final score\n",
    "        final_assessment = self.meta_agent.generate_meta_score(all_scores, essay, essay_prompt)\n",
    "        \n",
    "        # Compile complete evaluation result\n",
    "        evaluation_result = {\n",
    "            \"personas\": personas,\n",
    "            \"rubrics\": rubrics,\n",
    "            \"detailed_scores\": all_scores,\n",
    "            \"final_assessment\": final_assessment\n",
    "        }\n",
    "        \n",
    "        return evaluation_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = EssayEvaluationConfig(\n",
    "        n_agents=4,\n",
    "        max_score=5,\n",
    "        min_score=1,\n",
    "        max_holistic_score=100,\n",
    "        min_holistic_score=0,\n",
    "        feedback=True,\n",
    "        model_name=\"gpt-4o\"\n",
    "    )\n",
    "    \n",
    "    # Initialize system with OpenAI API key\n",
    "    system = EssayEvaluationSystem(\n",
    "        api_key=\"your_openai_api_key_here\",  # Replace with your API key\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Sample essay and prompt\n",
    "    essay_prompt = \"Discuss the impact of artificial intelligence on modern education.\"\n",
    "    \n",
    "    essay = \"\"\"\n",
    "    Artificial Intelligence: Reshaping Education\n",
    "    \n",
    "    The integration of artificial intelligence in education represents one of the most significant technological shifts in modern pedagogy. As AI systems become more sophisticated, they are transforming how students learn and how educators teach.\n",
    "    \n",
    "    Personalized learning stands as perhaps the most promising application of AI in education. Traditional classroom models often struggle to address the diverse needs of many students simultaneously. AI-powered platforms can analyze individual student performance, identify knowledge gaps, and adapt content delivery to match each student's learning pace and style. This personalization helps struggling students receive the additional support they need while allowing advanced learners to progress at an accelerated rate.\n",
    "    \n",
    "    Assessment is another area where AI demonstrates valuable potential. Automated grading systems can evaluate objective assignments instantaneously, freeing educators from time-consuming tasks and providing students with immediate feedback. More sophisticated AI tools are beginning to assess complex work like essays, analyzing factors including structure, argumentation, and coherence. While not replacing human judgment, these systems offer preliminary evaluations that help teachers manage large class loads more effectively.\n",
    "    \n",
    "    Administrative efficiency also improves with AI implementation. Institutions are deploying AI to streamline enrollment processes, schedule classes, and manage resources. These administrative applications allow educational institutions to operate more efficiently, potentially redirecting resources toward improving educational quality.\n",
    "    \n",
    "    However, the integration of AI in education raises important concerns. The digital divide may widen as schools with greater resources adopt advanced AI tools while underfunded institutions fall further behind. Questions about data privacy emerge as AI systems collect extensive information about students' learning behaviors and personal characteristics. Additionally, overreliance on technology might diminish crucial human elements of education—particularly the mentorship, inspiration, and emotional intelligence that skilled teachers provide.\n",
    "    \n",
    "    In conclusion, AI holds transformative potential for education while presenting significant challenges. The most successful educational futures will likely be those that thoughtfully integrate AI capabilities with irreplaceable human guidance, creating balanced learning environments that leverage technological advantages while preserving the essential human connections that give education its deepest value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate essay\n",
    "    evaluation_result = system.evaluate_essay(essay, essay_prompt)\n",
    "    \n",
    "    # Print formatted results\n",
    "    print(json.dumps(evaluation_result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
